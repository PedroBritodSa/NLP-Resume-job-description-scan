{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroBritodSa/NLP-Resume-job-description-scan/blob/main/NLP_project_resume_scan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Resume Scan and WordCloud\n",
        "- In this notebook, we will implement a simple NLP structure to scan resumes in docx format and compare with a job description\n",
        "- The code is structured in functions to facilitate usage and it is necessary to upload docx documents to see it working.  "
      ],
      "metadata": {
        "id": "YHHtvOOLUz1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXCmNA7KKPjl",
        "outputId": "ebb58453-68fb-4ae3-fa81-93a1757ec4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install docx2txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Loading some important libraries\n"
      ],
      "metadata": {
        "id": "SkM5xXp-ZLCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt #library to transform docx file into txt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer #Simple vectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity #library to search simality in text\n",
        "\n",
        "import string #to remove punctuations\n",
        "\n",
        "import re #regular expression library\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS # Set of stopwords to be removed\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "punc = string.punctuation # punctuations to be removed\n",
        "\n",
        "stopwords = ENGLISH_STOP_WORDS # stopwords to be removed"
      ],
      "metadata": {
        "id": "D8CtN6yaRYkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "id": "qrAjL57ZX_8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization\n",
        "\n",
        "- First we will create a tool to  visualize more frequent words in the douments. To do so, we will use the wordcloud library.\n",
        "- In order to analyse mostly the keywords, it is important to clean the unnecessary words, which we know as stopwords. So we will do it in the following functions"
      ],
      "metadata": {
        "id": "Xks-cWO7379k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to visualize the most frequency words in the data\n",
        "# docname must be changed by the name of the docx  that will be analized, for example: docname = \"name.docx\"\n",
        "def wc_func(docname):\n",
        "\n",
        "  data = docx2txt.process(docname)\n",
        "\n",
        "  data = ' '.join([word for word in data.split() if word.lower() not in stopwords]) # removing stopwords\n",
        "\n",
        "  wc = WordCloud(background_color=\"Black\", repeat=False, width = 1200 , height = 600) # Setting up the wordcloud\n",
        "\n",
        "  wc.generate(data) # applying the wc on the data\n",
        "\n",
        "  plt.figure(figsize = (20,20))\n",
        "\n",
        "  plt.axis(\"off\") # hide axis\n",
        "\n",
        "  plt.imshow(wc, interpolation=\"bilinear\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aXKvvG680wwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc_func(\"job_description.docx\")"
      ],
      "metadata": {
        "id": "KJZACWhsXWf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can also examine the words by their frequency in the text, which will be set up in the following function."
      ],
      "metadata": {
        "id": "egbOHg5WUKX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def most_freq_words(docname,number):\n",
        "\n",
        "    data = docx2txt.process(docname)\n",
        "\n",
        "    data = ''.join([char for char in data if char not in punc])\n",
        "\n",
        "\n",
        "    data = ' '.join([word for word in data.split() if word.lower() not in stopwords]) # removing stopwords\n",
        "\n",
        "    words = word_tokenize(data.lower())\n",
        "    # Count word frequency\n",
        "    word_count = Counter(words)\n",
        "\n",
        "    # Get the most common words and their frequencies\n",
        "    most_common_words = word_count.most_common(number)\n",
        "\n",
        "    # Convert the Counter object to a DataFrame\n",
        "    df = pd.DataFrame(most_common_words, columns=['Word', 'freq'])\n",
        "\n",
        "    df = df.sort_values(by='freq', ascending=True)\n",
        "\n",
        "    fig = px.bar(df, y='Word', x='freq', text_auto='.2s',\n",
        "            title=\"Most Common Words\", orientation = 'h')\n",
        "\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "mP0PCnNRBRSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_freq_words(\"job_description.docx\",18)"
      ],
      "metadata": {
        "id": "cJwf6_RqXY6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scanning the Resume\n",
        "\n",
        "- Finally, we can use the code sklearn library to create a count_matrix of our texts, and them calculate the similarity between them using the cosine similarity method"
      ],
      "metadata": {
        "id": "VL2rV7TZVS-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resume_scan(your_resume, job_description):\n",
        "\n",
        "  resume = docx2txt.process(your_resume) #Reading resume docx\n",
        "\n",
        "  job_des = docx2txt.process(job_description) #Reading job description docx\n",
        "\n",
        "  resume = ' '.join([word for word in resume.split() if word.lower() not in stopwords])\n",
        "\n",
        "  job_des = ' '.join([word for word in job_des.split() if word.lower() not in stopwords])\n",
        "\n",
        "  text = [resume, job_des] # Seting the two texts in a dataset\n",
        "\n",
        "  cv = CountVectorizer() # Creating the function CountVectorizer\n",
        "\n",
        "  count_matrix = cv.fit_transform(text) #Creating the count matrix\n",
        "\n",
        "  #Print the cosine similarity scores\n",
        "\n",
        "  print(\"\\nSimilarity Scores:\")\n",
        "\n",
        "  print(cosine_similarity(count_matrix))\n",
        "\n",
        "  print(\" \")\n",
        "\n",
        "  print(\" \")\n",
        "\n",
        "  print(\"\\nMatch Rate of: \" + str(your_resume) +\" and \" + str(job_description))\n",
        "\n",
        "  print(\" \")\n",
        "\n",
        "  print(\" \")\n",
        "\n",
        "  matchrate = cosine_similarity(count_matrix)[0][1]\n",
        "\n",
        "  matchpercen = round(matchrate*100,2) # rouded value\n",
        "\n",
        "  print(\"Match percentage between your resume and the job description is \" + str(matchpercen))\n"
      ],
      "metadata": {
        "id": "umLyHMbuPuh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_scan(\"resume.docx\",\"job_description.docx\")"
      ],
      "metadata": {
        "id": "SjDJGGSSeOvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}